\chapter{Introduction}
\label{cha:introduction}

\texttt{PAFFS} stands for "Protective Avionics Flash FS" and aims to use a minimum RAM footprint with the ability to manage multiple flashes. Some of the design ideas are inspired by the file system \texttt{JFFS3} \referenceDocument{JFFS3}, which was later extended \referenceDocument{JFFS3ex} and discontinued as predecessor of \texttt{UBIFS}.\\
The main differences to usual disk file systems are \texttt{out-of-place-writing}, a higher bit error rate and high deletion costs in terms of durability and speed. Keeping track of the files data chunks has to be different than just maintaining a big table on a disk, because the high frequented lookup table would be worn out earlier while other parts of flash will be nearly untouched. The standard approach of flash aware file systems (such as \texttt{YAFFS}) is to maintain a table-like structure in RAM and committing every chunk of new data to flash with an unique, increasing number, just like a list. This adds far more complexity to the file system as it has to scan the whole flash when mounting, but increases lifetime of the flash enormously. However, this RAM table grows linear with flash size, and thus does not scale for modern (> 2GB) flash chips. To solve this, the information has to be on flash. B+-Tree -> Section \ref{b+tree}.
Another big challenge is to reduce wear. Every change to data has to be written to another place and the old location has to be invalidated somehow. This is because the smallest unit of deletion is bigger (usually around 512 - 4086 times) than the smallest unit of a write operation. The common approach is to give every logical chunk an increasing version number. Because of the disadvantages pointed out earlier, this is not applied in \texttt{PAFFS}. Areas and address mapping -> Section \ref{areas}

\section{Fundamental structures}
\label{sec:funst}

\subsection{Areas}
\label{areas}
\paragraph{Overview}
An Area is a logical group of erase blocks. The number of contained blocks is a trade-off between RAM usage (less Areas) and garbage collection effectiveness (less overhead). Areas also separate different types of data and simplify handling of addresses after moving data. At the end of each Area, some amount of pages (usually not more than one) are reserved for an area summary (see chapter \ref{summary}). 
\begin{itemize}
	\item \texttt{AreaType} can be one of Superblock, Index, Data, (Journal). See table \ref{tab:areatypes}.
	\item Address split in \texttt{logical area n째} and \texttt{page n째}, to give way for a simple garbage collector (see  fig. \ref{fig:area_address}).
	\item \texttt{AreaMap} held in flash (but cached in RAM) translates between \texttt{logical area n째} and \texttt{physical area n째}. It also keeps record of corresponding types and usage statistics for garbage collection.
	\item\texttt{AreaStatus} can be one of \texttt{closed} (probably full, no guaranteed \texttt{area summary} in cache), \texttt{active} (\texttt{area summary} in cache, free pages) and \texttt{empty} (no \texttt{AreaType} set, no used pages). 
\end{itemize}

\subsection{Area types}
\begin{table}[htbp]
	\caption{Area types}
	\label{tab:areatypes}
	\begin{tabularx}{\textwidth}{lp{11cm}p{2.5cm}}
		\toprule
		Type & Description\\
		\midrule
		Superblock & One superblock area is on the first area of flash, the rest is dynamically allocated. It contains the anchor blocks as well as jump pads and a superpage. See chapter \ref{sub:chainedSB} \\
		Index & The index areas will only contain tree nodes (some of them storing inodes). See chapter \ref{b+tree}.\\
		Data & Data areas contain data chunks of files, directory entries and softlinks referenced by the index.\\
		Journal & The single journal area (some when) will contain uncommitted changes of the \texttt{area Summary}.\\
		\bottomrule
	\end{tabularx}
\end{table} 

\subsection{Area map / Addresses}
Due to the nature of flash, any deletion is delayed as long as possible. But when free space runs low, a garbage collector has to delete dirty pages while keeping addresses valid. This is why addresses consist of two parts; a logical area number, and page offset inside this area. To read a chunk of data at an address, the logical area number has to be translated to a physical area via the \texttt{AreaMap}. It is stored in the superpage (see chapter \ref{sub:chainedSB}), but is cached in RAM.\\
This area map grows linear in size with the number of areas which is defined as block count divided by area size.\\
The size of an area is a trade-off between low RAM usage (big areas) and a more efficient garbage collection (small areas). In this test environment, an area size of two erase blocks is chosen\footnote{Which should be the minimum size, because the anchor area has to be two blocks in size.}. With a normal 2GB flash chip\footnote{2048 Byte user data per page, 1024 pages per block, $\approx 2^21$ blocks.}, this configuration would use $(3+2+16+32+32) / 8 * 2^{21} / 2 = 11141120$ Byte $= 10,625$ MB RAM. Increasing the size of an area to 8 erase blocks would result in 2,65 MB usage, which would suit the requirements better.

\subsection{Area summary}
\paragraph{Overview}
\label{summary}
To distinguish between \texttt{free}, \texttt{used} and \texttt{dirty} pages, a per-area list is held in cache. The Information is used by the garbage collection copying only used pages to a new area, by the function looking for free pages for writing data or index chunks, and by runtime sanity checks. The Caching techniques are described in chapter \ref{ascache}. If the cache runs low or an unmount is requested, a packed area summary is written to the last pages of an area. Due to the nature of flash, this can only be done once until the area is garbage collected and therefore committable again.
\paragraph{Functionality}
For saving space, the information of \texttt{free}, \texttt{used} and \texttt{dirty} pages is truncated to only hold the information \texttt{used} or \texttt{other} for each page. The missing knowledge can be reattained by sweeping the whole area upon unpacking. If a page is empty, it is considered \texttt{free}; \texttt{dirty} if not.

\subsection{Garbage collection}
\paragraph{Overview}
The garbage collection needs at least one free area to copy valid data to. At first, one or more of the \texttt{closed} areas are inspected (as the \texttt{active} areas are still in use), sorted by textt{dirty} pages and eventually chosen for garbage collection. All valid pages are copied from the old area to the new area in their same relative positions. After that, the old area is erased, and the \texttt{AreaMap} is updated so that the logical areas swap their physical positions. The new area is marked \textit{active}, and the \texttt{AreaSummary} is deleted from Flash, thus giving way for a new commit of an area summary. If the previous way is not sufficient, the \texttt{garbage buffer} is given up for storing the more information. This procedure can not be reverted, and is reserved for \texttt{index} type only to enable a safe unmount without information loss.
\begin{figure}[ht]
	\centering\includegraphics[width=\textwidth]{Areas_address.png}
	\caption{Basic process of garbage collection with areas}
	\label{fig:area_address}
\end{figure}

\paragraph{Functionality}
Besides of looking for the most dirty blocks, the garbage collection also keeps the committed area summaries in mind. Depending on the mode (just looking for free space or trying to delete a committed area summary), it prefers or exclusively looks at areas with a committed area summary. This procedure helps the area summary cache freeing old entries.

\subsection{Area Summary Cache}
\label{ascache}
\paragraph{Overview}
The area summary caches read and write accesses to the area summaries of any area. Due to the limitation of RAM, the filesystem can't keep the status of every page in every area accessible. Every accessed area summary is loaded from cache (if previously committed to area) or created, until cache runs full. To free cache entries, the area containing the most dirty pages is committed to flash and deleted in cache. The minimum number of available cache entries is three, as two are needed for the \texttt{active} areas of \texttt{data} and \texttt{index} type, and one for simultaneously invalidating obsolete data. It is possible without the third cache entry, but the performance and long-term stability is very poor. A good cache size depends on simultaneously opened files for increased speed, however 5-10 entries will be sufficient for most Applications. Depending on the number of data pages per Area, a single entry uses between 32 and 256 bytes.

\paragraph{Functionality}
There are three different caching strategies implemented. When enough cache is available, every read or write access to a new area loads its contents to cache. If no free cache entries remain, and no cache entry is uncommitted (and therefore easy to delete), two possibilities remain. If a read access is queried, the corresponding area summary is loaded in a one-shot read without loading it to cache\footnote{Experiments show that there is less response-time and no improvement in wear leveling if the one-shot mode is used earlier, i.e. when no free cache entries remain without considering to commit areas.}. If it is a write access, the garbage collection is called in \texttt{urgent} mode to free any cached area from its obsolete (dirty) area summary\footnote{This also frees all other dirty pages as well.}. After that, a cache entry can be freed and the new one can be loaded. If garbage collection returns no committable Area (when free space is extremely low), the write access is ignored and an error is thrown. This only happens to an invalidation (former \texttt{used} page \texttt{dirty}), so the filesystem would not break, but gain unreachable pages that wont be deleted. The other transitions (from \texttt{free} to \texttt{used} and from any to \texttt{free}) are not possible to throw an error. The transition \texttt{free} to \texttt{used} only occurs on write actions in \texttt{active} areas which are guaranteed to have an entry in cache, the other one occurs in garbage collection freeing the whole area and thus deleting the whole cache entry.
Due to the layout of any commonly used processor, the size of an array element is mostly over 8 bit. To reduce the amount of RAM used, four 2 bit elements are packed into one byte. This quadruples the number of cacheable areas.

\subsection{Chained superblock}
\label{sub:chainedSB}
\paragraph{Overview}
Along with some static information, keeping an index requires having some sort of start point to find the first address of a table or the first position of a root node. The naive approach is having the first block contain this dynamic information. However this is bad practice, as it wears off the first (or first $n$) blocks enormously while leaving other places barely used. Another approach is to scan the whole flash for something with an unique sequential number, which is unacceptable as mount time scales linear with flash size.
The following structure of a chained superblock provides wear levelling of the first blocks and enables logarithmic mount time.
For a more detailed explanation see \referenceDocument{JFFS3} Chapter 4.

\paragraph{Functionality}
The first area of a flash contains two consecutive anchor blocks. The one page with the highest (i.e newest) number inside the anchor blocks is considered valid. This anchor page holds static information like file system version, number of blocks and the like as well as the address to the first jump pad located somewhere in a superblock area. These jump pads reference either a next pad or the superpage which is a page within a superblock area. This page holds frequently changed values like the address to the root node of the index tree and the area map.\\

When a change of the index tree has been committed to flash, the new address of the root node has to be written to a new superpage. The address of the new superpage is then appended to the last jump pad block. If this block is full, it will be erased and the new address is written to a new jump pad block. This requires the next higher-level jump pad to append the address of the new jump pad block, and so on. When one of the anchor blocks is full, first the anchor page is written to the other block, and then the first block is deleted.\\



\subsection{Tree Indexing}
\label{b+tree}
\paragraph{Overview}
B+Tree is a variation of the standard binary search tree with more than two keys per node and having all actual data only in leaves. This increases space for keys and therefore the fanout. Variable size, minimum tree height + 1\footnote{Check if true.}. Tree height is $log(n)$, and so is search, insert and delete $\mathcal{O}log(n)$. Because of \textit{out-of-place-write}, with every change to a single inode, the whole path up to the root node has to be updated (see figure \ref{fig:btreewandering}). To heavily decrease flash wear, a caching strategy is used. Any modification to the tree is held back in RAM as long as possible, until cache memory is running low. During cache flush all dirty nodes are committed to flash and freed in main memory. This caching can save 98\% of flash operations and speed up access times as well (see \referenceDocument{b+Tree}). Depending on cache flush strategy, all branch nodes are kept in cache while leaves are removed. This increases cache-hit ratio because there are less branches than leaves, but they branches more likely to be accessed again.

\begin{figure}[htp]
	\centering
	\begin{subfigure}[t]{.9\textwidth}
		\centering\includegraphics[width=\textwidth]{btree.png}
		\subcaption{Before}
	\end{subfigure}\\
	\begin{subfigure}[t]{.9\textwidth}
		\centering\includegraphics[width=\textwidth]{btree_2.png}
		\subcaption{After}
	\end{subfigure}
	\caption{\label{fig:btreewandering}B+-Tree after updating a single inode. The whole path has to be rewritten.}
\end{figure}

\begin{figure}[htp]
	\centering\includegraphics[width=\textwidth]{TreeCachePrint.PNG}
	\caption{\label{fig:TreeCachePrint}Example output of a cache content visualization. Notice the \texttt{dirty}, \texttt{locked} and \texttt{inherited lock} flags.}
\end{figure}

\subsection{Inodes}
\label{inode}
\paragraph{Overview}
Can be file, directory and softlink. Points to data chunks in a way like \texttt{EXT3} does. See Table \ref{tab:pinode. The maximum filesize depends on the data bytes per page; with 512 Byte per page it is calculated as 
$512 * (11 + (512/8)^1 + (512/8)^2 + (512/8)^3) = 136353280 Byte \approx 138 MiB$. At a page width of 2084, the max filesize is $\approx 34 GiB$.
\begin{table}[htbp]
	\caption{Inode contents}
	\label{tab:pinode}
	\begin{tabularx}{\textwidth}{lp{11cm}p{2.5cm}}
		\toprule
		Value & Description\\
		\midrule
		\texttt{pInode\_no} & Unique number of inode. \\
		\texttt{pInode\_type} & One of \texttt{file}, \texttt{directory} or \texttt{link}.\\
		\texttt{permission} & Global read, write and execute permissions. Changeable via \texttt{chmod}.\\
		\texttt{created} & Unix time stamp of file creation.\\
		\texttt{modified} & Unix time stamp of last file write access.\\
		\texttt{size} & Size of user data in bytes. \\
		\texttt{reserved size} & Space user data is actually occupying in bytes (multiple of chunk size).\\
		\texttt{direct pointer} & 11 direct pointers to user data chunks.\\
		\texttt{indirect pointers} & One of each first, second and third layer indirection pointers pointing to data chunks with more pointers.\\
		\bottomrule
	\end{tabularx}
\end{table} 

\section{Tests}
\begin{flushleft}
	
\end{flushleft}
This section is under development since the filesystem is unfinished. Meanwhile look at fancy pictures:

\begin{figure}[htp]
	\centering\includegraphics[width=\textwidth]{paffs__fileIO_annotations.png}
	\caption{\label{fig:fileIO} Status of simulated flash after 125 read/write cycles on a single file. Without a garbage collection, the number of rewrites is limited by flash size.\\TODO: Change "contents of file" to "Dirty or used Pages of file"}
\end{figure}
\begin{figure}[htp]
	\centering\includegraphics[width=\textwidth]{paffs__fileIO_GC_annotations.png}
	\caption{\label{fig:GC} Status of simulated flash after 1562658 read/write cycles on a single file. This result is dependend on block erasure count.}
\end{figure}
\begin{figure}[htp]
	\centering\includegraphics[width=\textwidth]{yaffs_paffs_comparison_WL.png}
	\caption{\label{fig:fileIOCompare} Comparison of wear levelling performance as a function of flash size between PAFFS and YAFFS1.}
\end{figure}
