
\chapter{Introduction}
\label{cha:introduction}

\texttt{PAFFS} stands for "Protective Aeronautics Flash FS" and aims to use a minimum RAM footprint with the ability to manage multiple flashes. Most of the design ideas are inspired by the file system \texttt{JFFS3} \referenceDocument{JFFS3}, which was later extended \referenceDocument{JFFS3ex} and discontinued as predecessor of \texttt{UBIFS}.\\
The main differences to usual disk file systems are \texttt{out-of-place-writing}, a higher bit error rate and high deletion costs in terms of durability and speed. Keeping track of the files data chunks has to be different than just maintaining a big table on a disk, because the high frequented lookup table would be worn out earlier while other parts of flash will be nearly untouched. The standard approach of flash aware file systems (such as \texttt{YAFFS}) is to maintain a table-like structure in RAM and committing every chunk of new data to flash with an unique, increasing number, just like a list. This adds far more complexity to the file system as it has to scan the whole flash when mounting, but increases lifetime of the flash enormously. However, this RAM table grows linear with flash size, and thus does not scale for modern (> 2GB) flash chips. To solve this, the information has to be on flash. B+-Tree -> Section \ref{b+tree}.
Another big challenge is to reduce wear. Every change to data has to be written to another place and the old location has to be invalidated somehow. This is because the smallest unit of deletion is bigger (usually around 512 - 4086 times) than the smallest unit of a write operation. The common approach is to give every logical chunk an increasing version number. Because of the disadvantages pointed out earlier, this is not applicable to \texttt{PAFFS}. Areas and address mapping -> Section \ref{areas}

\section{Fundamental structures}
\label{sec:funst}


\subsection{Areas}
\label{areas}
\paragraph{Overview}
To separate between different Types of Data. Is the logical combination of two or more erase blocks.
\begin{itemize}
	\item \texttt{AreaType} can be one of Superblock, Index, Data, (Journal). See table \ref{tab:areatypes}.
	\item Address split in \texttt{logical area n째} and \texttt{page n째}, to give way for an easy garbage collector (see  fig. \ref{fig:area_address})
	\item \texttt{AreaMap} held in flash (but cached in RAM) translates between \texttt{logical area n째} and \texttt{physical area n째}
	\item \texttt{AreaMap} also keeps record of corresponding types and usage statistics for garbage collection
	\item\texttt{AreaStatus} can be one of \texttt{closed}, \texttt{active} and \texttt{empty}. 
\end{itemize}

\paragraph{Area types}
\begin{table}[htbp]
\caption{Area types}
\label{tab:areatypes}
\begin{tabularx}{\textwidth}{lp{11cm}p{2.5cm}}
\toprule
Type & Description\\
\midrule
Superblock & One superblock area is automatically on the first area of flash, the rest is dynamically allocated. It contains the anchor blocks as well as jump pads and a superpage. See chapter \ref{sub:chainedSB} \\
Index & The index areas will contain only tree nodes (including inodes). See Chapter \ref{b+tree}.\\
Data & Data areas contain data chunks of files, directories and softlinks referenced by the index.\\
Journal & The single journal area (some when) will contain uncommitted changes to the index.\\
\bottomrule
\end{tabularx}
\end{table} 

\paragraph{Area map / Addresses}
Due to the nature of flash, any deletion is delayed as long as possible. But when free space runs low, a garbage collector has to delete dirty pages while keeping addresses valid. This is why addresses consist of two parts; a logical area number, and page offset inside this area. To read chunk of data at an address, the logical area number has to be translated to a physical area via the \texttt{AreaMap}. It is stored in the superpage (see chapter \ref{sub:chainedSB}), but is cached in RAM.\\
This area map grows linear in size with area count which is by itself depended by block count divided by area size.\\
The size of an area is a trade-off between low RAM usage (big areas) and a more efficient garbage collection (small areas). In this test environment, an area size of two erase blocks is chosen\footnote{Which should be the minimum size, because the anchor area has to be two blocks in size.}. With a normal 2GB flash chip\footnote{2048 Byte user data per page, 1024 pages per block, $\approx 1*10^6$ blocks.}, this configuration would use $(3+2+16+32+32+32+32) / 8 * (2048*1024*1024*1024) / (2048 * 1024) / 2 = 19529728 / 2 = 9764864$ Byte $= 9,31$ MB RAM. Increasing the size of an area to 8 erase blocks would result in 2,3 MB usage, which would suit the requirements better.

\paragraph{Area summary}
The area map also contains a pointer to an area summary, which is an array of the per-page information if it was \texttt{free}, \texttt{used} or \texttt{dirty}. This area summary completely in RAM if the area is active, but is written to the first page in an area when it is getting closed. It is being used as information where to find the next free page and for the garbage collector to copy only valid chunks. The reference may only be valid if area status is active.

\subsection{Garbage collection}
\paragraph{Functionality}
The garbage collection needs at least one free area to copy valid data to. At first, one or more of the \textit{closed} and therefore usually full areas are inspected (as their \texttt{AreaSummary}s are not cached) and eventually chosen for garbage collection. All valid pages are copied from the old area to the new area in their same relative positions. After that, the old area is erased, and the \texttt{AreaMap} is updated so that the physical areas swap their logical numbers. The new area is marked \textit{active}, and the \texttt{AreaSummary} is held in RAM. If the previous way is not sufficient, \textit{active} and therefore non-empty areas are chosen. This would only be done at the very end of space because it is not as efficient.

\begin{figure}[ht]
  \centering\includegraphics[width=\textwidth]{Areas_address.png}
  \caption{Basic process of garbage collection with areas}
  \label{fig:area_address}
\end{figure}

\subsection{Chained superblock}
\label{sub:chainedSB}
\paragraph{Overview}
Along with some static information, keeping an index requires having some sort of start point to find the first address of a table or the first position of a root node. The naive approach is having the first block contain this dynamic information. However this is bad practice, as it wears off the first (or first $n$) enormously in contrast to other places being barely used. Another approach is to scan the whole flash for something with an unique sequential number, which is unacceptable as mount time scales linear with flash size.
The following structure of a chained superblock provides wear levelling of the first blocks and enables logarithmic mount time.
For a more detailed explanation see \referenceDocument{JFFS3} Chapter 4.

\paragraph{Functionality}
The first area of a flash contains two consecutive anchor blocks. The one page with the highest (i.e newest) number inside the anchor blocks is considered valid. This anchor page holds static information like file system version, number of blocks and the like as well as the address to the first jump pad located somewhere in a superblock area. these jump pads reference either a next pad or the superpage which is a page within a superblock area. This page holds frequently changed values like the address to the root node of the index tree and the area map.\\

When a change of the index tree has been committed to flash, the new address of the root node has to be written to a new superpage. The address of the new superpage is then appended to the last jump pad block. If this block is full, it will be erased and the new address is written to a new jump pad block. This requires the next higher-level jump pad to append the address of the new jump pad block, and so on. When one of the anchor blocks is full, first the anchor page is written to the other block, and then the first block is deleted.\\



\subsection{Tree Indexing}
\label{b+tree}
\paragraph{Overview}
B+Tree is a variation of the standard binary search tree with more keys per node. Another difference is having all actual data only in leaves. This increases space for keys and therefore the fanout. Variable size, minimum tree height + 1\footnote{Check if true.}. Tree height is $log(n)$, and so is search, insert and delete $\mathcal{O}log(n)$. Because of \textit{out-of-place-write}, with every change to a single inode, the whole path up to the root node has to be updated (see figure \ref{fig:btreewandering}). To heavily decrease flash wear, a caching strategy is used. Any modification to the tree is held back in RAM as long as possible, until main memory space is running low. Only now all nodes marked as dirty are committed to flash and freed in main memory. This caching can save 98\% of flash operations and speeds up access as well (see \referenceDocument{b+Tree}).

\begin{figure}[htp]
	\centering
	\begin{subfigure}[t]{.9\textwidth}
		\centering\includegraphics[width=\textwidth]{btree.png}
		\subcaption{Before}
	\end{subfigure}\\
	\begin{subfigure}[t]{.9\textwidth}
		\centering\includegraphics[width=\textwidth]{btree_2.png}
		\subcaption{After}
	\end{subfigure}
	\caption{\label{fig:btreewandering}B+-Tree after updating a single inode. The whole path has to be rewritten.}
\end{figure}

\subsection{Inodes}
\label{inode}
\paragraph{Overview}
Can be file, directory and softlink. Points to data chunks in a way like \texttt{EXT3} does. See Table \ref{tab:pinode}. It is possible to use pointer space for very small files or directories (14 * 4 Byte).
\begin{table}[htbp]
\caption{Inode contents}
\label{tab:pinode}
\begin{tabularx}{\textwidth}{lp{11cm}p{2.5cm}}
\toprule
Value & Description\\
\midrule
\texttt{pInode\_no} & Unique number of inode. \\
\texttt{pInode\_type} & One of \texttt{file}, \texttt{directory} or \texttt{link}.\\
\texttt{permission} & Global read, write and execute permissions. Changeable via \texttt{chmod}.\\
\texttt{created} & Unix time stamp of file creation.\\
\texttt{modified} & Unix time stamp of last file write access.\\
\texttt{size} & Size of user data in bytes. \\
\texttt{reserved size} & Space user data is actually occupying in bytes (multiple of chunk size).\\
\texttt{direct pointer} & 11 direct pointers to user data chunks.\\
\texttt{indirect pointers} & One of each first, second and third layer indirection pointers pointing to data chunks with more pointers.\\
\bottomrule
\end{tabularx}
\end{table} 


%\begin{lstlisting}[caption={Example C++ Code Listing},language=c++]
%enum TelemetryGeneration
%{
%    noTelemetry,
%    generateTelemetry
%};
%
%class Example
%{
%public:
%    Example(TelemetryGeneration telemetry,
%            uint8_t* buffer,
%            uint16_t length);
%    ...
%};
%
%Example object1(noTelemetry, 0, 0);
%
%uint8_t buffer[16];
%Example object1(generateTelemetry, buffer, size(buffer));
%\end{lstlisting}